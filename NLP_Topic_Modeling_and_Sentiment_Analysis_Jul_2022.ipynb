{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Topic Modeling and Sentiment_Analysis_Jul_2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Code snippet # 0 - getting the data from web server\n",
        "url = 'https://raw.githubusercontent.com/nvamsimohan/DallasDSA/main/Amazon%20reviews.txt'"
      ],
      "metadata": {
        "id": "cguUEs8kwWKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code snippet # 1- importing the 'requests' package to read the text data from the url\n",
        "import requests\n",
        "reviews = requests.get(url)\n",
        "text = reviews.text"
      ],
      "metadata": {
        "id": "onbckTsRwN5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWofGv6V4p63"
      },
      "source": [
        "# Code snippet # 2  Displaying the content of the reviews file\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX44Qg4-4uym"
      },
      "source": [
        "# Code snippet # 3 - Importing required NLP libraries\n",
        "import nltk \n",
        "nltk.download('punkt') \n",
        "\n",
        "from nltk import sent_tokenize # this helps to split text into Sentences\n",
        "from nltk import word_tokenize # this helps to split text into individual Words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45e-8Po4xbO"
      },
      "source": [
        "# Code snippet #4 - Tokenize the text by sentences :\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "#How many sentences are there? :\n",
        "print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUs4fEOU47VO"
      },
      "source": [
        "#Code snippet #5 Print the sentences :\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-I7cwcZ49Jd"
      },
      "source": [
        "#Code snippet #6 Tokenize the text with words :\n",
        "words = word_tokenize(text)\n",
        "\n",
        "#How many words are there? :\n",
        "print(len(words))\n",
        "print(\"\\n\")\n",
        "\n",
        "#Print words :\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-lGXC-Q4_wh"
      },
      "source": [
        "# Code snippet #7 Exploratory Analysis on the Text\n",
        "\n",
        "# Import required libraries\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Find the frequency of the words\n",
        "fdist = FreqDist(words)\n",
        "# Printing the frequency\n",
        "print(fdist)\n",
        "\n",
        "# Find the 10 most common words \n",
        "fdist_10 = fdist.most_common(10)\n",
        "print(fdist_10)\n",
        "\n",
        "# Plot the graph for fdist :\n",
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqTpLqzo5DOO"
      },
      "source": [
        "# Code snippet #8 - Text Data Cleaning - taking out the punctuations\n",
        "\n",
        "# Empty list to store words:\n",
        "words_no_punc = []\n",
        "\n",
        "#Removing punctuation marks :\n",
        "for w in words:\n",
        "    if w.isalpha():\n",
        "        words_no_punc.append(w.lower())\n",
        "\n",
        "#Print the words without punctuation marks :\n",
        "print (words_no_punc)\n",
        "print (\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QiQBU9RBrk"
      },
      "source": [
        "# Code snippet #9 Exploratory Analysis on the Text\n",
        "#Length :\n",
        "print(len(words_no_punc))\n",
        "\n",
        "#Frequency distribution :\n",
        "fdist = FreqDist(words_no_punc)\n",
        "\n",
        "# Displaying the top 10 common words\n",
        "fdist.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsW0GsEy5IWv"
      },
      "source": [
        "# Code snippet #11  Text Data Cleaning loading the stopwords library\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#List of stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-78_BjmPbn2"
      },
      "source": [
        "# Code snippet #12 Text Data Cleaning removing the stopwords from the text\n",
        "\n",
        "#Empty list to store clean words :\n",
        "clean_words = []\n",
        "\n",
        "for w in words_no_punc:\n",
        "    if w not in stopwords:\n",
        "        clean_words.append(w)\n",
        "        \n",
        "print(clean_words)\n",
        "print(\"\\n\")\n",
        "print(len(clean_words))\n",
        "\n",
        "#Frequency distribution :\n",
        "fdist = FreqDist(clean_words)\n",
        "fdist.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atTQ7dXY5Ms1"
      },
      "source": [
        "# Code snippet # 14 Library to form wordcloud :\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#Library to plot the wordcloud :\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Generating the wordcloud :\n",
        "wordcloud = WordCloud().generate(text)\n",
        "#Plot the wordcloud :\n",
        "plt.figure(figsize = (14, 14)) \n",
        "plt.imshow(wordcloud) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViARhYkh7QSF"
      },
      "source": [
        "#  Code snippet # 15 Sentiment Analysis\n",
        "# First, we import the relevant modules from the NLTK library\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv1NnCmb7Txd"
      },
      "source": [
        "#  Code snippet #16 - Initialize VADER so we can use it within our Python script\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEZ5V8jf7VgL"
      },
      "source": [
        "#  Code snippet #17 - Calling the polarity_scores method on sid and passing in the message_text outputs a dictionary \n",
        "# with negative, neutral, positive, and compound scores for the input text\n",
        "scores = sid.polarity_scores(text)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}